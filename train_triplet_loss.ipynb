{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Masking, Dense, Dropout, Flatten, Lambda, LSTM, BatchNormalization, Activation, Concatenate, concatenate\n",
    "import pandas as pd\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD\n",
    "# !pip install 'fsspec>=0.3.3'\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random as rd\n",
    "import random\n",
    "\n",
    "'''\n",
    "Author: Ahmed Anu Wahab\n",
    "Date: February 2022\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Usable Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = 'ml_features/'\n",
    "out_path = 'triplets/'\n",
    "\n",
    "'''\n",
    "This function gets the usable Users.\n",
    "NOTE: Usable users must have 15 sentences\n",
    "'''\n",
    "def get_usable_participants(in_path, out_path):\n",
    "    column_names = [\"user\", \"sentence_id\", \"sentence_number\", \"m\", \"ud\", \"dd\", \"uu\", \"id\"]\n",
    "    user_files = os.listdir(in_path)\n",
    "    usable_par, unusable_par = [],[]\n",
    "    for user_file in user_files:\n",
    "        try:\n",
    "            data = pd.read_csv(in_path + user_file) # Get the data in the user file\n",
    "            user_sentence_number = data.sentence_number.unique() # Get array-list of unique sentence number\n",
    "            sentence_num=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14] # Reference list of sentence number\n",
    "            '''Check if they are subset of each other'''\n",
    "            if (sentence_num and user_sentence_number.tolist()) == sentence_num:\n",
    "                usable_par.append(user_file[0:-4])\n",
    "            else:\n",
    "                unusable_par.append(user_file[0:-4])\n",
    "        except:\n",
    "            print('Error found, file skipped!!!')\n",
    "            continue\n",
    "    print('USABLE: ', len(usable_par))\n",
    "    print('UNUSABLE: ', len(unusable_par))\n",
    "    '''Compress and export'''\n",
    "    np.savez_compressed(out_path + 'usable.npz', usable_par)\n",
    "    np.savez_compressed(out_path + 'unusable.npz', unusable_par)\n",
    "\n",
    "# Call the function----------------------\n",
    "get_usable_participants(in_path, out_path)\n",
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Train and Test Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select Train and Test users from the usable users\n",
    "'''\n",
    "all_users = np.load('triplets/usable.npz')['arr_0']\n",
    "np.random.shuffle(all_users) # Shuffle all users\n",
    "train_users, test_users = all_users[:68000], all_users[68000:] # First 68,000 for training; remaining 100,000 for test\n",
    "np.savez_compressed('triplets/train_users.npz', train_users) # Save train users\n",
    "np.savez_compressed('triplets/test_users.npz', test_users) # Save test users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(gen_user, imp_user, seq_length, features):\n",
    "    gen_sentences=random.sample(list(range(15)),2) # Select 2 random sentence number\n",
    "    A_sentence = gen_sentences[0] # First is the anchor senetence number\n",
    "    P_sentence = gen_sentences[1] # Second is the positive senetence number\n",
    "    N_sentence=rd.randint(15) # Select 1 random imp sentence number\n",
    "    \n",
    "    '''Get user sentences'''\n",
    "    gen_df = pd.read_csv('ml_features/' + gen_user + '.csv')\n",
    "    \n",
    "    '''Clean the data by removing duration greater than 5 secs. They are considered outliers'''\n",
    "    A_arr = gen_df[gen_df.sentence_number==A_sentence].iloc[:, 3:]\n",
    "    A_arr.drop(A_arr.index[(abs(A_arr.m)>5)|(abs(A_arr.ud)>5)|(abs(A_arr.dd)>5)|(abs(A_arr.uu)>5)], inplace=True)\n",
    "    A_arr = A_arr.values\n",
    "    \n",
    "    P_arr = gen_df[gen_df.sentence_number==P_sentence].iloc[:, 3:]\n",
    "    P_arr.drop(P_arr.index[(abs(P_arr.m)>5)|(abs(P_arr.ud)>5)|(abs(P_arr.dd)>5)|(abs(P_arr.uu)>5)], inplace=True)\n",
    "    P_arr = P_arr.values\n",
    "    \n",
    "    imp_df = pd.read_csv('ml_features/' + imp_user + '.csv')\n",
    "    N_arr = imp_df[imp_df.sentence_number==N_sentence].iloc[:, 3:]\n",
    "    N_arr.drop(N_arr.index[(abs(N_arr.m)>5)|(abs(N_arr.ud)>5)|(abs(N_arr.dd)>5)|(abs(N_arr.uu)>5)], inplace=True)\n",
    "    N_arr = N_arr.values\n",
    "    \n",
    "    '''Truncate if number of keystrokes in the sample is greater than the specified sequence length'''\n",
    "    if len(A_arr) >= seq_length:\n",
    "        A = A_arr[:seq_length, :]\n",
    "    if len(P_arr) >= seq_length:\n",
    "        P = P_arr[:seq_length, :]\n",
    "    if len(N_arr) >= seq_length:\n",
    "        N = N_arr[:seq_length, :]\n",
    "    \n",
    "    '''Pad with zeros if number of keystrokes in the sample is less than the specified sequence length'''\n",
    "    if len(A_arr) < seq_length:\n",
    "        A = np.concatenate([A_arr, np.zeros((seq_length-len(A_arr), features))])\n",
    "    if len(P_arr) < seq_length:\n",
    "        P = np.concatenate([P_arr, np.zeros((seq_length-len(P_arr), features))])\n",
    "    if len(N_arr) < seq_length:\n",
    "        N = np.concatenate([N_arr, np.zeros((seq_length-len(N_arr), features))])\n",
    "\n",
    "    return A,P,N\n",
    "\n",
    "\n",
    "def gen_batch(batch_size, seq_length, features=5):\n",
    "    '''Get the train users'''\n",
    "    train_users = np.load('triplets/train_users.npz')['arr_0']\n",
    "    '''Batch data and label placeholders'''\n",
    "    batch = np.zeros(shape=(batch_size, 3, seq_length, features)) # Example (512, 3, 70, 5)\n",
    "    y = np.zeros(shape=(batch_size,))\n",
    "    '''Forever loop for generating batch data'''\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            users = random.sample(train_users.tolist(), 2) # Select two random user\n",
    "            gen_user, imp_user = users[0], users[1] # First is set as genuine user, the other is set as an imposter\n",
    "            if gen_user == imp_user: # Sanity check\n",
    "                print('Same user was chosen for gen and imp')\n",
    "            A,P,N = get_sample(gen_user, imp_user, seq_length,features) # Generate batch sample\n",
    "            batch[i,0,:,:] = A\n",
    "            batch[i,1,:,:] = P\n",
    "            batch[i,2,:,:] = N\n",
    "        yield (batch[:,0,:,:], batch[:,1,:,:], batch[:,2,:,:]), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    '''Base model.\n",
    "    '''\n",
    "    input_ = Input(shape=input_shape)\n",
    "    x = Masking(mask_value=0.)(input_)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LSTM(128,recurrent_dropout=0.20, return_sequences=True)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LSTM(128,recurrent_dropout=0.20)(x)\n",
    "    return Model(input_, x)\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    '''Triplet loss function\n",
    "    '''\n",
    "    margin = 1.5\n",
    "    out_size = 128\n",
    "    anchor, positive, negative = y_pred[0,:], y_pred[1,:], y_pred[2,:]\n",
    "    positive_dist = K.sum(K.square(anchor - positive), axis=-1)\n",
    "    negative_dist = K.sum(K.square(anchor - negative), axis=-1)\n",
    "    loss = K.sum(K.relu(margin + positive_dist - negative_dist))\n",
    "    return loss\n",
    "\n",
    "'''\n",
    " Allocate GPU space for training.\n",
    " This allocates 0.8% of GPU memory. This memory space size can be changed as need be.\n",
    "'''\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create checkpoint, the best over training loss.'''\n",
    "checkpoint = ModelCheckpoint('triplets/new_model/triplet_model(alpha=1.5).h5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Get the model ready\n",
    "length=70\n",
    "steps_per_epoch=100\n",
    "epoch=150\n",
    "seq_per_batch = 512\n",
    "input_shape = (length,5)\n",
    "base_network = create_base_network(input_shape)\n",
    "input_anc,input_pos, input_neg = Input(shape=input_shape),Input(shape=input_shape),Input(shape=input_shape)\n",
    "processed_anc,processed_pos, processed_neg = base_network(input_anc),base_network(input_pos),base_network(input_neg)\n",
    "output = tf.stack([processed_anc, processed_pos, processed_neg])\n",
    "model = Model([input_anc, input_pos, input_neg], output)\n",
    "adam=keras.optimizers.Adam(learning_rate=0.05, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss=triplet_loss, optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "hst = model.fit(gen_batch(seq_per_batch,length), steps_per_epoch=steps_per_epoch, epochs=epoch, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(list(hst.history.values())[0], '-b', label=\"Train loss\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
